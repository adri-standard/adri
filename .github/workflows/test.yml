name: Main Branch CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  # Quality checks run first and fast-fail if issues found
  quality-gate:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Run code quality checks
      run: |
        echo "ðŸŽ¨ Running Black formatting check..."
        black --check --diff adri/ tests/

        echo "ðŸ“¦ Running isort import sorting check..."
        isort --check-only --diff adri/ tests/

        echo "ðŸ” Running flake8 linting..."
        flake8 adri/ tests/

        echo "ðŸ”¬ Running mypy type checking..."
        mypy adri/ --ignore-missing-imports

    - name: Run security scan
      run: |
        echo "ðŸ”’ Running Bandit security scan..."
        bandit -r adri/ -f json -o bandit-report.json || true
        bandit -r adri/

        echo "ðŸ›¡ï¸ Running Safety vulnerability check..."
        safety check --continue-on-error || echo "Safety check completed with warnings"

  test:
    needs: quality-gate
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Run all tests with coverage
      run: |
        pytest tests/ -v --cov=adri --cov-report=xml --cov-report=term-missing --cov-fail-under=90

    - name: Upload coverage to Codecov
      if: matrix.python-version == '3.11'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  performance:
    needs: [quality-gate, test]
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' || github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark pytest-timeout tabulate pyyaml requests

    - name: Download Previous Benchmark Results
      if: github.event_name == 'pull_request'
      continue-on-error: true
      run: |
        echo "ðŸ“¥ Attempting to download previous benchmark results..."
        python scripts/download_previous_benchmark.py \
          --repo "${{ github.repository }}" \
          --branch "${{ github.base_ref || 'main' }}" \
          --workflow "test.yml" \
          --artifact "benchmark-results-main" \
          --output "previous-benchmark.json" \
          --token "${{ secrets.GITHUB_TOKEN }}" || echo "No previous benchmark found"

    - name: Run performance benchmarks with timeout
      run: |
        echo "âš¡ Running performance benchmarks with timeout protection..."
        pytest tests/test_benchmarks.py -v \
          --performance \
          --benchmark-only \
          --benchmark-json=benchmark.json \
          --timeout=60 \
          --timeout-method=thread \
          --no-cov || true

    - name: Add metadata to benchmark results
      run: |
        python -c "
        import json
        from datetime import datetime

        # Load benchmark results
        with open('benchmark.json', 'r') as f:
            data = json.load(f)

        # Add metadata
        data['commit_sha'] = '${{ github.sha }}'
        data['branch'] = '${{ github.ref_name }}'
        data['timestamp'] = datetime.utcnow().isoformat()
        data['workflow'] = 'test.yml'

        # Save updated results
        with open('benchmark.json', 'w') as f:
            json.dump(data, f, indent=2)
        "

    - name: Compare benchmark results
      if: always()
      run: |
        echo "ðŸ“Š Comparing benchmark results..."

        # Check if previous benchmark exists
        if [ -f "previous-benchmark.json" ]; then
          python scripts/compare_benchmarks.py \
            benchmark.json \
            --previous previous-benchmark.json \
            --thresholds .github/benchmark-thresholds.yml \
            --output comparison-report.md \
            --github-output || echo "Comparison completed with warnings"
        else
          echo "No previous benchmark found, checking thresholds only..."
          python scripts/compare_benchmarks.py \
            benchmark.json \
            --thresholds .github/benchmark-thresholds.yml \
            --output comparison-report.md \
            --github-output || echo "Threshold check completed"
        fi

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          benchmark.json
          comparison-report.md

    - name: Store benchmark for main branch
      if: github.ref == 'refs/heads/main'
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-main
        path: benchmark.json
        retention-days: 30

    - name: Comment PR with benchmark comparison
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      continue-on-error: true
      with:
        script: |
          const fs = require('fs');

          let comment = '## âš¡ Performance Benchmark Summary\n\n';

          // Add comparison report if it exists
          if (fs.existsSync('comparison-report.md')) {
            const report = fs.readFileSync('comparison-report.md', 'utf8');
            comment += report;
          } else {
            comment += 'No comparison data available.\n';
          }

          // Find and update existing comment or create new one
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });

          const botComment = comments.find(comment =>
            comment.user.type === 'Bot' && comment.body.includes('Performance Benchmark Summary')
          );

          if (botComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: comment
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
          }

    - name: Check performance thresholds
      run: |
        echo "ðŸ” Checking performance against thresholds..."

        # Run threshold check using comparison script
        python scripts/compare_benchmarks.py \
          benchmark.json \
          --thresholds .github/benchmark-thresholds.yml \
          --enforce || {
          echo "âš ï¸ Performance check completed with warnings"
          # Don't fail the build yet (enforcement disabled initially)
          exit 0
        }

  build-validation:
    needs: [quality-gate, test]
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine

    - name: Build package
      run: |
        python -m build

    - name: Check package
      run: |
        twine check dist/*

    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: dist-${{ github.sha }}
        path: dist/

  summary:
    needs: [quality-gate, test, performance, build-validation]
    runs-on: ubuntu-latest
    if: always()

    steps:
    - name: CI Summary
      run: |
        echo "## ðŸš€ CI Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Quality Gate: ${{ needs.quality-gate.result }}" >> $GITHUB_STEP_SUMMARY
        echo "### Tests: ${{ needs.test.result }}" >> $GITHUB_STEP_SUMMARY
        echo "### Performance: ${{ needs.performance.result }}" >> $GITHUB_STEP_SUMMARY
        echo "### Build: ${{ needs.build-validation.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [[ "${{ needs.quality-gate.result }}" == "success" && "${{ needs.test.result }}" == "success" && "${{ needs.build-validation.result }}" == "success" ]]; then
          echo "âœ… **All checks passed!** Ready for merge." >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Some checks failed.** Please review and fix issues." >> $GITHUB_STEP_SUMMARY
        fi
